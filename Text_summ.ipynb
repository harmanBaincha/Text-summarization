{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_summ.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULvP0esR2oPg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e03cf1b8-78f3-4b8c-806b-302e939f5c2d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_dir = \"/content/gdrive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYFlTK5f3Gos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "1f2f368c-86ba-48bc-f150-51c10cd07918"
      },
      "source": [
        "!pip install sumy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sumy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/20/8abf92617ec80a2ebaec8dc1646a790fc9656a4a4377ddb9f0cc90bc9326/sumy-0.8.1-py2.py3-none-any.whl (83kB)\n",
            "\r\u001b[K     |████                            | 10kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 2.4MB/s \n",
            "\u001b[?25hCollecting breadability>=0.1.20\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/2d/bb6c9b381e6b6a432aa2ffa8f4afdb2204f1ff97cfcc0766a5b7683fec43/breadability-0.1.20.tar.gz\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from sumy) (3.2.5)\n",
            "Collecting pycountry>=18.2.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/73/6f1a412f14f68c273feea29a6ea9b9f1e268177d32e0e69ad6790d306312/pycountry-20.7.3.tar.gz (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from sumy) (2.23.0)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from breadability>=0.1.20->sumy) (3.0.4)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.6/dist-packages (from breadability>=0.1.20->sumy) (4.2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.0.2->sumy) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (2020.6.20)\n",
            "Building wheels for collected packages: breadability, pycountry\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21684 sha256=643401ce7c8f559d6fc41eab9299081273a6ce3e4b4407cfaf1283ed9222cbad\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/4d/a1/510b12c5e65e0b2b3ce539b2af66da0fc57571e528924f4a52\n",
            "  Building wheel for pycountry (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-20.7.3-py2.py3-none-any.whl size=10746863 sha256=4f6ad4d289fbdf741f7cf2c87c2c4966496b2be6813e87a2f51ec0d1af8c4484\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/4e/a6/be297e6b83567e537bed9df4a93f8590ec01c1acfbcd405348\n",
            "Successfully built breadability pycountry\n",
            "Installing collected packages: breadability, pycountry, sumy\n",
            "Successfully installed breadability-0.1.20 pycountry-20.7.3 sumy-0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcFlIgRS2yDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sumy.parsers.html import HtmlParser\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "import glob\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkMyuw213dFo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2398ca26-875e-4f43-94af-089e6b3fe845"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDyh_rmC3D5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_req=[]\n",
        "for file_name in glob.iglob('/content/gdrive/My Drive/text_summ/*', recursive=True): \n",
        "    with open(file_name) as f:\n",
        "        dict_cont_org=''\n",
        "        for line in f.readlines():\n",
        "            name,conv=line.split(':')\n",
        "            dict_cont_org=dict_cont_org+conv\n",
        "#         sentens=nltk.tokenize.sent_tokenize(dict_cont_org)\n",
        "    dict_req.append(dict_cont_org)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSxDShCI3VGJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "66d1c7e5-4a9b-47d1-fdc8-e578eace860d"
      },
      "source": [
        "num_sentences_in_summary = 10\n",
        "# url = \"https://en.wikipedia.org/wiki/Automatic_summarization\"\n",
        "# parser = HtmlParser.from_url(url, Tokenizer(\"english\"))\n",
        "# parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
        "# parser = PlaintextParser.from_string(\"Check this out.\", Tokenizer(LANGUAGE))\n",
        "summarizer_list=(\"TextRankSummarizer:\",\"LexRankSummarizer:\",\"LuhnSummarizer:\",\"LsaSummarizer\") #list of summarizers\n",
        "summarizers = [TextRankSummarizer(), LexRankSummarizer(), LuhnSummarizer(), LsaSummarizer()]\n",
        "\n",
        "for i,summarizer in enumerate(summarizers):\n",
        "    print(summarizer_list[i])\n",
        "    for doc in dict_req:\n",
        "        parser = PlaintextParser.from_string(doc, Tokenizer(\"english\"))\n",
        "        for sentence in summarizer(parser.document, num_sentences_in_summary):\n",
        "            print((sentence))\n",
        "        print(\"-\"*30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TextRankSummarizer:\n",
            "So up to this point, I was thinking that image function that you're using in the get functions, actually from the P i l library, but it's from fast they I regional Ivers, So its a difference in years.\n",
            "You think that's up to you because It's just the me and the nationals working on this.\n",
            "I mean, I'm I just They just, you know, just out of curiosity, say yeah, OK. Yeah, I just I just didn't want to waste.\n",
            "If you think you can do that, that's that's why I actually just like before the call.\n",
            "I didn't find many links, but I mean, it's so if you If you think that's the way to go, maybe you just want to spend some time on.\n",
            "It did died if I was three closer and defects before, but there the what I shared with you, so they make this.\n",
            "This is actually this is at the data data source level, so it zig So we have data, said Data Loader, and then data Barnes and Data Loader is actually just doing.\n",
            "So instead of in that shape, just just move that in that shape to the law.\n",
            "so just uh, can we get the dimensions of uh, so do not so I don't know if there's.\n",
            "Yeah, You know, these will be in touch because it's weird, and I shouldn't be like that, and it might be.\n",
            "------------------------------\n",
            "I don't know if you check out the bullshit the Excelsior that they had, but But I add a different attributes and then is doing some sort of, um I'm distracting it now.\n",
            "Okay, then you could, if you could update me on, that would be good in the latest was like the last time we met, Uh Friday because I think it was decided they were not much discussed.\n",
            "really the model becomes, but I don't remember within the past a second part of the first part, but the model becomes better if you feed like a sentence, but reversing the order so a simple example.\n",
            "That's I'm happy with the results of having with the results of, you know, dresses with Come up when I look for dresses is a pretty similar, like before.\n",
            "so as hospitalization before somebody's Don't open and I the art Jake Exide, because the extension, um so who it cost if you could give me, like, the highlight of the ones that which one which they didn't.\n",
            "Then it is likely that that is a bad image in which get just check with the Syria about, like some fighting package installations that you need to do to make it work on.\n",
            "So one of the ways multi that I adapted and another one was the ones that I wanted to remove them, you know?\n",
            "I don't think so, But I can't, because the way it trains just the problem with that is doesn't capture some off the distances correctly, right?\n",
            "maybe you could finish your No, that's and, um my plan is so according to the Friday's results, the decision shows The Weighted Lords for coefficient off 10 is a master model, but just based on the STG, we don't know the model.\n",
            "he's looking 15. exactly, but just like we with that once he created widgets you need to make I think, and also handle it sometimes because I like it a lot of images, right?\n",
            "------------------------------\n",
            "LexRankSummarizer:\n",
            "So the\n",
            "You think that's up to you because It's just the me and the nationals working on this.\n",
            "so maybe like this, But then I think that in that so\n",
            "If you think you can do that, that's that's why I actually just like before the call.\n",
            "I didn't find many links, but I mean, it's so if you If you think that's the way to go, maybe you just want to spend some time on.\n",
            "It did died if I was three closer and defects before, but there the what I shared with you, so they make this.\n",
            "it's that.\n",
            "so just uh, can we get the dimensions of uh, so do not so I don't know if there's.\n",
            "That's, uh yeah, I remember image that data.\n",
            "Just needed to Yeah, that that's okay.\n",
            "------------------------------\n",
            "Is it?\n",
            "I think I know what you mean, But, um, here's the more training because I'm happy with the results.\n",
            "Probably so that's why it doesn't so there is no Well, one thing that it creates do like I know till at all Green Maybe black There.\n",
            "You think that those okay, but yeah, there is also looking to this area and see if that package would help up.\n",
            "Yeah, actually, if you could just start search about that, I think there should be using ah, for methods of doing the other people.\n",
            "We can see the accuracy is and or is it okay or can I Should I continue to do the model together?\n",
            "And also like a Why for you is like 50 more.\n",
            "That's that's what it is.\n",
            "That's what it is.\n",
            "I\n",
            "------------------------------\n",
            "LuhnSummarizer:\n",
            "So up to this point, I was thinking that image function that you're using in the get functions, actually from the P i l library, but it's from fast they I regional Ivers, So its a difference in years.\n",
            "I mean, I'm I just They just, you know, just out of curiosity, say yeah, OK. Yeah, I just I just didn't want to waste.\n",
            "If you think you can do that, that's that's why I actually just like before the call.\n",
            "I didn't find many links, but I mean, it's so if you If you think that's the way to go, maybe you just want to spend some time on.\n",
            "It did died if I was three closer and defects before, but there the what I shared with you, so they make this.\n",
            "This is actually this is at the data data source level, so it zig So we have data, said Data Loader, and then data Barnes and Data Loader is actually just doing.\n",
            "No, I don't think it's actually I don't think Did you also have this line from print A town they?\n",
            "so just uh, can we get the dimensions of uh, so do not so I don't know if there's.\n",
            "Yeah, You know, these will be in touch because it's weird, and I shouldn't be like that, and it might be.\n",
            "I will look into death and try to see if I can say we will see the M.L.. emergency if properly inside.\n",
            "------------------------------\n",
            "Okay, then you could, if you could update me on, that would be good in the latest was like the last time we met, Uh Friday because I think it was decided they were not much discussed.\n",
            "really the model becomes, but I don't remember within the past a second part of the first part, but the model becomes better if you feed like a sentence, but reversing the order so a simple example.\n",
            "That's I'm happy with the results of having with the results of, you know, dresses with Come up when I look for dresses is a pretty similar, like before.\n",
            "Maybe we could have, like, a different kind of pleading in distance or when its color on then go but Docker Well, what do you mean, Like is missing, so there is no connections.\n",
            "And I think when it comes down to, um, the similarity of these when red is higher, it's probably used when they talk when in the text talking about blue like Indo sentence.\n",
            "Then it is likely that that is a bad image in which get just check with the Syria about, like some fighting package installations that you need to do to make it work on.\n",
            "Because we can indirectly discomfort things, like other other types off the oh, the material India Once that you mentioned because for colors from the vector, space has been considered like it should be considered.\n",
            "But But listen, the way it picks it up is it has to do again with your data and how correlated thes naming came up to get so like blue and red was more frequent and seeing together from model perspective, then blue and great or green.\n",
            "maybe you could finish your No, that's and, um my plan is so according to the Friday's results, the decision shows The Weighted Lords for coefficient off 10 is a master model, but just based on the STG, we don't know the model.\n",
            "I think it's good to put just all the yard together and then redistribute it to make sure, like, 00 stuff at first, Like a we find your girls like enough euros.\n",
            "------------------------------\n",
            "LsaSummarizer\n",
            "Oh, and this the state a bunch thing is driving us crazy.\n",
            "We got much closer yesterday, but now they're so Oh, it's asking for password.\n",
            "Let's just give in the past or five and 70 to us.\n",
            "used up the aisle after A.B.. Then you told me to comment on.\n",
            "It's a different type is saying to Yes, I'm gonna work out.\n",
            "How can we save in umpire image area and say because number.\n",
            "Yeah, because you're passing it as a bad rate and in your own words.\n",
            "so maybe for saving, you need to permeate the game because that's they.\n",
            "but this No, they conversion to bash is done in the data Loader.\n",
            "Okay, I will try to look more into that issue which show bash.\n",
            "------------------------------\n",
            "I just said whether you're venting squeezed, running back e have made, like, a few more minutes.\n",
            "Okay, then you could, if you could update me on, that would be good in the latest was like the last time we met, Uh Friday because I think it was decided they were not much discussed.\n",
            "Um, this 40. percent of global, uh not useful and it's pretty slow because have to be small.\n",
            "That means it's not picking up colors, which kind of makes sense, because when we're feeding text.\n",
            "It's just based on, like how often they are kind of appearing together in the data.\n",
            "was, uh rgb spectrum, but that you also have to think about because, like audio map Docker in Who.\n",
            "Yeah, actually, if you could just start search about that, I think there should be using ah, for methods of doing the other people.\n",
            "But that just means looking for a very culture specific date back more than anything else.\n",
            "So I think if you could just give those aesthetics is also like a current lead and that be nice.\n",
            "That would be amazing because triple it said he actually doing a lot of data and way behind.\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}